[llm]
# --- OPTION 1: OpenAI (Recommended for best results) ---
# model = "gpt-4-turbo"
# api_key = "sk-YOUR-OPENAI-API-KEY-HERE"
# base_url = "https://api.openai.com/v1/chat/completions"

# --- OPTION 2: Ollama (Free, Local) ---
# Ensure you have Ollama running: `ollama serve`
# And pull a model: `ollama pull mistral`
model = "mistral"
api_key = "ollama"  # Not used but required filler
base_url = "http://localhost:11434/v1/chat/completions"

# --- OPTION 3: Other (Claude, etc via request proxy) ---
# Check documentation for base_url

max_tokens = 4096
temperature = 0.1

[chunking]
chunk_size = 500  # Detailed extraction
overlap = 50

[standardization]
enabled = true
use_llm_for_entities = true

[inference]
enabled = true
use_llm_for_inference = true
apply_transitive = true

[visualization]
edge_smooth = true
