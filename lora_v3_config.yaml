model: mlx-community/Llama-3.2-3B-Instruct-4bit
train: true
data: ./training_output_v3
adapter_path: ./adapters_v3
fine_tune_type: lora
batch_size: 4
iters: 600
learning_rate: 2e-5
steps_per_eval: 100
steps_per_report: 50
save_every: 200
max_seq_length: 2048
seed: 42
val_batches: 25
lora_parameters:
  rank: 8
  dropout: 0.0
  scale: 20.0
